#!/bin/bash

# SIMD: => https://software.intel.com/sites/landingpage/IntrinsicsGuide/

### KESCH (Haswell E52690v3)
### git clone https://github.com/ursache/HPC-hacks.git HPC-hacks.git

# TODO: HPC-hacks.git => LAPLACE (sse, avx)

INTELROOT=/global/opt/intel/16.0.0.109
source $INTELROOT/bin/compilervars.sh intel64 
source $INTELROOT/advisor_xe_2016.1.0.423501/advixe-vars.sh
export ADVISOR_XE_2015_DIR=$ADVISOR_XE_2016_DIR
export PATH=/apps/common/UES/sandbox/jgp/exoopen/bin:$PATH
which advixe-gui   

cd /scratch-shared/mch/scratch/piccinal/HPC-hacks.git/Kernels/dgemm-kernels/

## man icc / icc -help codegen
use -mavx # ftn -v + craype-haswell
# -mcode where code = avx,sse4.2,4.1,3,2,nothing
# -march=core-avx2
# -xcode where code = SSE4.2,4.1,3,2,CORE-AVX2    AVX512=MIC

#    #    ##       #    #    #  ######
##   #   #  #      #    #    #  #
# #  #  #    #     #    #    #  #####
#  # #  ######     #    #    #  #
#   ##  #    #     #     #  #   #
#    #  #    #     #      ##    ######
## naive:
# make    # -mavx !!!
# ##  Size: 512 512 512 = 134 217 728
# time -p ./dgemm-naive   #                  Gflop/s:  0.861195 real 0.41
# time -p ./dgemm-avx     # 24.136081 GFlops Gflop/s: 20.6933   real 0.02
# time -p ./dgemm-avx2    # 37.997364 GFlops Gflop/s: 25.0875   real 0.02
# 
# ## Size: 2048 2048 2048 = 8 589 934 592
# time -p ./dgemm-naive 2048 #                  Gflop/s: 0.562666 real 30.62
# time -p ./dgemm-avx   2048 # 24.730055 GFlops Gflop/s: 24.1182  real 0.80
# time -p ./dgemm-avx2  2048 # 38.056364 GFlops Gflop/s: 35.9749  real 0.57

### compile:
CFLAGS="-O3 -openmp -g -mavx" # -msse4.2
icc -c $CFLAGS dgemm.c
icc -c $CFLAGS dgemm-naive.c
icc dgemm.o dgemm-naive.o $CFLAGS -mkl=sequential 
mv a.out naive.mavx

### run:
time -p ./naive.mavx 1024  # Gflop/s: 0.96694 real 2.24
# time -p ./naive.sse42 1024 # Gflop/s: 1.03581 real 2.31
# 
time -p ./naive.mavx 2048  # Gflop/s: 0.280814 real 61.49
# time -p ./naive.sse42 2048 # Gflop/s: 0.273114 real 63.21

### advisor_annotate:
cat dgemm-naive.c
# --------------------------------
# --------------------------------
# --------------------------------
# {{{
//author gilles.fourestey@epfl.ch
#ifdef _advisor_annotate
#include "advisor-annotate.h"
#endif

void dgemm( const int M, const int N, const int K, const double alpha, const double *A, const int lda, const double *B, const int ldb, const double beta, double* C, const int ldc)
{
        int i, j, k;

#ifdef _advisor_annotate
ANNOTATE_SITE_BEGIN(dgemmnaive);
#endif

        for (j = 0; j < N; ++j)
                for (i = 0; i < M; ++i)
                {
                        double cij = 0.; 
#ifdef _advisor_annotate
    ANNOTATE_ITERATION_TASK(kloop);
#endif
                        for (k = 0; k < K; ++k) 
                                cij += A[i + k*lda]*B[k + j*ldb];
                        C[j*ldc + i] += cij;    
                }
#ifdef _advisor_annotate
ANNOTATE_SITE_END();
#endif
}
# }}}
# --------------------------------
# --------------------------------
# --------------------------------

#### annotate/compile:
CFLAGS="-O3 -openmp -g -mavx -D_advisor_annotate -I$ADVISOR_XE_2016_DIR/include -L$ADVISOR_XE_2016_DIR/lib64 -ladvisor -ldl"
icc -c $CFLAGS dgemm.c
icc -c $CFLAGS dgemm-naive.c
icc dgemm.o dgemm-naive.o $CFLAGS -mkl=sequential 
mv a.out naive.mavx+advisor
# export PATH=/apps/common/UES/sandbox/jgp/exoopen/bin:$PATH

#### annotate/run:
time -p ./naive.mavx+advisor 1024 # Gflop/s: 0.964313 real 2.25
time -p ./naive.mavx+advisor 2048 # Gflop/s: 0.433811 real 39.70
# run with 2048 (sinon trop court ?)
advixe-gui &

### jeusel:
icc -O0 -g -mavx -D_DGEMM_SSE4 -D_DGEMM_AVX2 dgemm-main.c -mkl=sequential && ./a.out 
	# size_array=1000000000 x=2.000000 y=1.000000
	# saxpy_naive took 2.528467 seconds x=2.000000 y=21.000000
	# saxpy_sse4 took 1.738535 seconds x=2.000000 y=43.000000
	# saxpy_avx2 took 1.063628 seconds x=2.000000 y=67.000000


  ####    ####   #    #  #####      #    #       ######  #####
 #    #  #    #  ##  ##  #    #     #    #       #       #    #
 #       #    #  # ## #  #    #     #    #       #####   #    #
 #       #    #  #    #  #####      #    #       #       #####
 #    #  #    #  #    #  #          #    #       #       #   #
  ####    ####   #    #  #          #    ######  ######  #    #

 #####   ######  #####    ####   #####    #####
 #    #  #       #    #  #    #  #    #     #
 #    #  #####   #    #  #    #  #    #     #
 #####   #       #####   #    #  #####      #
 #   #   #       #       #    #  #   #      #
 #    #  ######  #        ####   #    #     #
# Neil:
        https://www.youtube.com/watch?v=QdfQW8x2c8Y
        /project/csstaff/courses/CSCS_multicore_2012/tutorials/Vect/Simple
        /project/csstaff/piccinal/.sph/.sph-flow.git/CSCS/compiler_reports
# Goal: show that "compiler vectorized naive loop" is slower than "avx2 hand written loop"

  ####   #    #  #    #
 #    #  ##   #  #    #
 #       # #  #  #    #
 #  ###  #  # #  #    #
 #    #  #   ##  #    #
  ####   #    #   ####
# GNU ---------------
cd /project/csstaff/piccinal/.sph/.sph-flow.git/src_stage/GNU/
# => .rpt
#FLAGS="-O2 -g -march=native -ftree-vectorize -fopt-info-vec-missed"
#gcc -D_GEMM_NAIVE_XY  $FLAGS naive.c -oAxy &> o_Axy
#gcc -D_GEMM_AVX2_XY   $FLAGS naive.c -oBxy &> o_Bxy
#srun -n1 --exclusive ./Axy # naive: t=0.732007 n=1000000000 x=2.000000 y=25.000000 z=0.000000
#srun -n1 --exclusive ./Bxy #  avx2: t=0.710107 n=1000000000 x=2.000000 y=25.000000 z=0.000000 

# ---------
# Jeusel:
gcc -D_GEMM_NAIVE_XY -mavx2 -march=native naive.c
srun -n1 --exclusive ./a.out # naive: t=2.719085 n=1000000000 x=2.000000 y=25.000000 z=0.000000
gcc -D_GEMM_AVX2_XY -mavx2 -march=native naive.c
srun -n1 --exclusive ./a.out # avx2: t=1.035836 n=1000000000 x=2.000000 y=25.000000 z=0.000000
# 
gcc -D_GEMM_NAIVE_XY -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out  # t=2.714553 ! ~ -O0
gcc -D_GEMM_AVX2_XY -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out   # t=1.027768
# -O2 ?:
gcc -D_GEMM_NAIVE_XY -O2 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out  # t=0.698657 ! similar timings
gcc -D_GEMM_AVX2_XY -O2 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out   # t=0.728430
# -O3 ?:
gcc -D_GEMM_NAIVE_XY -O3 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out  # t=0.728044 ! similar timing
gcc -D_GEMM_AVX2_XY -O3 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out   # t=0.734874
# -O1 ?:
gcc -D_GEMM_NAIVE_XY -O1 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out  # t=0.731025 ! similar timings
gcc -D_GEMM_AVX2_XY -O1 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out   # t=0.726342
# -O0 ?: <---------------
gcc -D_GEMM_NAIVE_XY -O0 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out  # t=2.712323 ! avx2 faster 
gcc -D_GEMM_AVX2_XY -O0 -mavx2 -march=native -ftree-vectorize naive.c && srun -n1 --exclusive ./a.out   # t=1.037863
# -O0 + -fopt-info-vec-missed: requires -O1 minimum
#
# BRISI CN: E5-2690 v3 @ 2.60GHz Haswell / gcc/4.9.3
# $1 -mavx2 -march=native -ftree-vectorize
# n=1000000000 x=2.000000 y=25.000000 z=0.000000
# for i in *.-O0*;do ./res.sh $i ;done
./0.sh "-D_GEMM_NAIVE_XY -O0"           #naive: t=2.716306
./0.sh "-D_GEMM_SSE4_XY  -O0"           #sse4: t=1.666279 
./0.sh "-D_GEMM_AVX2_XY  -O0"           #avx2: t=1.052850 
./0.sh "-D_GEMM_NAIVE_XY -O0 -g"        #naive: t=2.715812
./0.sh "-D_GEMM_SSE4_XY  -O0 -g"        #sse4: t=1.685013 
./0.sh "-D_GEMM_AVX2_XY  -O0 -g"        #avx2: t=1.047704 
# 050times:
# 250times:
o_exe.-D_GEMM_AVX2_XY..-O0.-g. / min=0.9899 avg=0.9902 max=0.9928 max-min=0.0029 #fast250gnu 2.6565/0.9902=2.7x, 1.6335/0.9902=1.6x
   o_exe.-D_GEMM_AVX2_XY..-O0. / min=0.9893 avg=0.9908 max=0.9952 max-min=0.0059
o_exe.-D_GEMM_NAIVE_XY.-O0.-g. / min=2.6582 avg=2.6611 max=2.6617 max-min=0.0035
   o_exe.-D_GEMM_NAIVE_XY.-O0. / min=2.6537 avg=2.6565 max=2.6569 max-min=0.0032
o_exe.-D_GEMM_SSE4_XY..-O0.-g. / min=1.6352 avg=1.6365 max=1.6368 max-min=0.0016
   o_exe.-D_GEMM_SSE4_XY..-O0. / min=1.6326 avg=1.6335 max=1.6338 max-min=0.0012
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O1"           
./0.sh "-D_GEMM_SSE4_XY  -O1"           
./0.sh "-D_GEMM_AVX2_XY  -O1"           
./0.sh "-D_GEMM_NAIVE_XY -O1 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O1 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O1 -g"        
# 050times:
# 250times:
o_exe.-D_GEMM_AVX2_XY..-O1.-g. / min=0.6575 avg=0.6577 max=0.6581 max-min=0.0006
   o_exe.-D_GEMM_AVX2_XY..-O1. / min=0.6569 avg=0.6573 max=0.6576 max-min=0.0007
o_exe.-D_GEMM_NAIVE_XY.-O1.-g. / min=0.6570 avg=0.6576 max=0.6581 max-min=0.0011
   o_exe.-D_GEMM_NAIVE_XY.-O1. / min=0.6564 avg=0.6569 max=0.6573 max-min=0.0009
o_exe.-D_GEMM_SSE4_XY..-O1.-g. / min=0.6316 avg=0.6319 max=0.6322 max-min=0.0006 #fast250gnu but all very close
   o_exe.-D_GEMM_SSE4_XY..-O1. / min=0.6391 avg=0.6394 max=0.6426 max-min=0.0035
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O2"           
./0.sh "-D_GEMM_SSE4_XY  -O2"           
./0.sh "-D_GEMM_AVX2_XY  -O2"           
./0.sh "-D_GEMM_NAIVE_XY -O2 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O2 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O2 -g"        
# 050times:
# 250times: for i in o_exe.*O2*;do ./res.sh $i ;done
o_exe.-D_GEMM_AVX2_XY..-O2.-g. / min=0.6499 avg=0.6503 max=0.6508 max-min=0.0009
   o_exe.-D_GEMM_AVX2_XY..-O2. / min=0.6541 avg=0.6548 max=0.6702 max-min=0.0161
o_exe.-D_GEMM_NAIVE_XY.-O2.-g. / min=0.6530 avg=0.6534 max=0.6651 max-min=0.0121
   o_exe.-D_GEMM_NAIVE_XY.-O2. / min=0.6512 avg=0.6518 max=0.6610 max-min=0.0098
o_exe.-D_GEMM_SSE4_XY..-O2.-g. / min=0.6377 avg=0.6382 max=0.6385 max-min=0.0008 #fast250gnu but all very close
   o_exe.-D_GEMM_SSE4_XY..-O2. / min=0.6390 avg=0.6394 max=0.6397 max-min=0.0007
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O3"           
./0.sh "-D_GEMM_SSE4_XY  -O3"           
./0.sh "-D_GEMM_AVX2_XY  -O3"           
./0.sh "-D_GEMM_NAIVE_XY -O3 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O3 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O3 -g"        
# 050times:
# 250times: for i in o_exe.*O3*;do ./res.sh $i ;done
o_exe.-D_GEMM_AVX2_XY..-O3.-g. / min=0.6547 avg=0.6552 max=0.6559 max-min=0.0012
   o_exe.-D_GEMM_AVX2_XY..-O3. / min=0.6534 avg=0.6538 max=0.6543 max-min=0.0009
o_exe.-D_GEMM_NAIVE_XY.-O3.-g. / min=0.6536 avg=0.6541 max=0.6546 max-min=0.001
   o_exe.-D_GEMM_NAIVE_XY.-O3. / min=0.6575 avg=0.6580 max=0.6658 max-min=0.0083
o_exe.-D_GEMM_SSE4_XY..-O3.-g. / min=0.6384 avg=0.6389 max=0.6452 max-min=0.0068
   o_exe.-D_GEMM_SSE4_XY..-O3. / min=0.6382 avg=0.6387 max=0.6391 max-min=0.0009 #fast250gnu but all very close
# ---------
# can compiler explain ?

-mavx2
# gcc -ftree-vectorize -fopt-info-vec-missed -ftree-vectorizer-verbose=2 -D_GEMM_NAIVE -O3 -march=native naive.c
gcc -fopt-info     -D_GEMM_NAIVE -O3 -march=native naive.c -oA &> eff_naive_info.rpt
# gcc -fopt-info-vec -D_GEMM_NAIVE -O3 -march=native naive.c &> eff_naive_infovec.rpt
gcc -fopt-info     -D_GEMM_AVX2 -O3 -march=native naive.c -oB &> eff_avx2_info.rpt
# gcc -fopt-info-vec -D_GEMM_AVX2 -O3 -march=native naive.c &> eff_avx2_infovec.rpt

FLAGS="-O2 -ftree-vectorize -fopt-info-vec-missed -march=native" # -fopt-info
# deprecated option -ftree-vectorizer-verbose => https://gcc.gnu.org/viewcvs/gcc?view=revision&revision=204244
gcc -D_GEMM_NAIVE $FLAGS naive.c -oA &> eff_naive_info_O2.rpt
sort -k1 eff_naive_info_O2.rpt |uniq
	# naive.c:54:29: note: Failed to SLP the basic block.
	# naive.c:54:29: note: SLP: step doesn't divide the vector-size.
	# naive.c:54:29: note: Unknown alignment for access: *(x_8(D) + (sizetype) ((long unsigned int) i_60 * 4))
	# naive.c:54:29: note: Unknown alignment for access: *(y_5(D) + (sizetype) ((long unsigned int) i_60 * 4))
	# naive.c:54:29: note: Unknown alignment for access: *x_8(D)
	# naive.c:54:29: note: Unknown alignment for access: *xx_5
	# naive.c:54:29: note: Unknown alignment for access: *y_5(D)
	# naive.c:54:29: note: Unknown alignment for access: *yy_7
	# naive.c:54:29: note: not vectorized: failed to find SLP opportunities in basic block.
	# naive.c:54:29: note: not vectorized: no vectype for stmt: vect__10.73_70 = MEM[(float *)vectp_x.71_68];
	# naive.c:54:29: note: not vectorized: no vectype for stmt: vect__32.9_65 = MEM[(float *)vectp_xx.7_63];
	# naive.c:54:29: note: not vectorized: not enough data-refs in basic block.
	# naive.c:56:1: note: not vectorized: not enough data-refs in basic block.


gcc -D_GEMM_AVX2  $FLAGS naive.c -oB &> eff_avx2_info_O2.rpt
	# naive.c:66:9: note: bad data references.
	# naive.c:66:9: note: not vectorized: no vectype for stmt: _16 = MEM[(__m256 * {ref-all})_9];
	# naive.c:66:9: note: not vectorized: no vectype for stmt: _32 = MEM[(__m256 * {ref-all})_31];
	# naive.c:67:26: note: not vectorized: no vectype for stmt: _16 = MEM[(__m256 * {ref-all})_9];
	# naive.c:67:26: note: not vectorized: no vectype for stmt: _32 = MEM[(__m256 * {ref-all})_31];
	# naive.c:67:26: note: not vectorized: not enough data-refs in basic block.

# regression: 502*_craype-haswell_fma_PrgEnv-*.sh
GNU:   -march=core-avx2
PGI:   -tp=haswell

  ####    #####  #####   ######    ##    #    #
 #          #    #    #  #        #  #   ##  ##
  ####      #    #    #  #####   #    #  # ## #
      #     #    #####   #       ######  #    #
 #    #     #    #   #   #       #    #  #    #
  ####      #    #    #  ######  #    #  #    #
# /apps/daint/5.2.UP02/hwloc/1.10.1/gnu_482/bin/lstopo-no-graphics
L3  L#1 (30MB)
L2 L#12 (256KB) 
L1d L#12 (32KB) + L1i L#12 (32KB)
# stream-clean.git => from L2 to L3 (?):

16:
Array size = 983040 (elements).
Triad: 28.559     0.000859     0.000826     0.000899
Memory per array = 7.5 MiB (= 0.0 GiB).

17:
Array size = 1966080 (elements).
Triad: 13.855     0.003428     0.003406     0.003510
Memory per array = 15.0 MiB (= 0.0 GiB).

    #    #    #   #####  ######  #
    #    ##   #     #    #       #
    #    # #  #     #    #####   #
    #    #  # #     #    #       #
    #    #   ##     #    #       #
    #    #    #     #    ######  ######
# INTEL ---------------
# BRISI CN: E5-2690 v3 @ 2.60GHz Haswell / intel/15.0.1.133 => brisi02 !!! / cscs regression: -xCORE-AVX2 
# $1 -xHOST -mkl=sequential -qopt-report=5 -qopt-report-phase=vec / n=1000000000 x=2.000000 y=5857.000000 z=0.000000
./0.sh "-D_GEMM_NAIVE_XY -O0"
./0.sh "-D_GEMM_SSE4_XY  -O0"
./0.sh "-D_GEMM_AVX2_XY  -O0"
./0.sh "-D_GEMM_NAIVE_XY -O0 -g"
./0.sh "-D_GEMM_SSE4_XY  -O0 -g"
./0.sh "-D_GEMM_AVX2_XY  -O0 -g"
# 050times:
# 250times:
o_exe.-D_GEMM_AVX2_XY..-O0.-g. / min=0.9824 avg=0.9835 max=0.9856 max-min=0.0032 #fast250intel  # intel
   o_exe.-D_GEMM_AVX2_XY..-O0. / min=0.9994 avg=1.0000 max=1.0004 max-min=0.001                 # intel
o_exe.-D_GEMM_NAIVE_XY.-O0.-g. / min=2.4171 avg=2.4196 max=2.4201 max-min=0.003                 # intel
   o_exe.-D_GEMM_NAIVE_XY.-O0. / min=2.4168 avg=2.4192 max=2.4199 max-min=0.0031                # intel 
o_exe.-D_GEMM_SSE4_XY..-O0.-g. / min=1.6707 avg=1.6726 max=1.6732 max-min=0.0025                # intel
   o_exe.-D_GEMM_SSE4_XY..-O0. / min=1.6720 avg=1.6745 max=1.6788 max-min=0.0068                # intel
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O1"           
./0.sh "-D_GEMM_SSE4_XY  -O1"           
./0.sh "-D_GEMM_AVX2_XY  -O1"           
./0.sh "-D_GEMM_NAIVE_XY -O1 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O1 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O1 -g"        
# 250times:
o_exe.-D_GEMM_AVX2_XY..-O1.-g. / min=0.6498 avg=0.6502 max=0.6568 max-min=0.007  #fast250intel  # intel
   o_exe.-D_GEMM_AVX2_XY..-O1. / min=0.6541 avg=0.6545 max=0.6550 max-min=0.0009                # intel
o_exe.-D_GEMM_NAIVE_XY.-O1.-g. / min=0.6871 avg=0.6903 max=0.6933 max-min=0.0062                # intel
   o_exe.-D_GEMM_NAIVE_XY.-O1. / min=0.6859 avg=0.6880 max=0.6903 max-min=0.0044                # intel 
o_exe.-D_GEMM_SSE4_XY..-O1.-g. / min=0.6520 avg=0.6523 max=0.6526 max-min=0.0006                # intel
   o_exe.-D_GEMM_SSE4_XY..-O1. / min=0.6604 avg=0.6608 max=0.6612 max-min=0.0008                # intel
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O2"           
./0.sh "-D_GEMM_SSE4_XY  -O2"           
./0.sh "-D_GEMM_AVX2_XY  -O2"           
./0.sh "-D_GEMM_NAIVE_XY -O2 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O2 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O2 -g"        
# 250times: for i in o_exe.*O2*;do ./res.sh $i ;done
o_exe.-D_GEMM_AVX2_XY..-O2.-g. / min=0.6527 avg=0.6532 max=0.6539 max-min=0.0012                        # intel
   o_exe.-D_GEMM_AVX2_XY..-O2. / min=0.6536 avg=0.6547 max=0.6562 max-min=0.0026                        # intel
o_exe.-D_GEMM_NAIVE_XY.-O2.-g. / min=0.6482 avg=0.6489 max=0.6572 max-min=0.009  #fast250intel but ...  # intel
   o_exe.-D_GEMM_NAIVE_XY.-O2. / min=0.6528 avg=0.6533 max=0.6537 max-min=0.0009                        # intel 
o_exe.-D_GEMM_SSE4_XY..-O2.-g. / min=0.6614 avg=0.6618 max=0.6621 max-min=0.0007                        # intel
   o_exe.-D_GEMM_SSE4_XY..-O2. / min=0.6622 avg=0.6626 max=0.6629 max-min=0.0007                        # intel
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O3"           
./0.sh "-D_GEMM_SSE4_XY  -O3"           
./0.sh "-D_GEMM_AVX2_XY  -O3"           
./0.sh "-D_GEMM_NAIVE_XY -O3 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O3 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O3 -g"        
# 250times: for i in o_exe.*O3*;do ./res.sh $i ;done
o_exe.-D_GEMM_AVX2_XY..-O3.-g. / min=0.6530 avg=0.6535 max=0.6539 max-min=0.0009 #fast250intel but ...  # intel
   o_exe.-D_GEMM_AVX2_XY..-O3. / min=0.6561 avg=0.6570 max=0.6669 max-min=0.0108                        # intel
o_exe.-D_GEMM_NAIVE_XY.-O3.-g. / min=0.6540 avg=0.6544 max=0.6549 max-min=0.0009                        # intel
   o_exe.-D_GEMM_NAIVE_XY.-O3. / min=0.6535 avg=0.6539 max=0.6543 max-min=0.0008                        # intel 
o_exe.-D_GEMM_SSE4_XY..-O3.-g. / min=0.6633 avg=0.6635 max=0.6684 max-min=0.0051                        # intel
   o_exe.-D_GEMM_SSE4_XY..-O3. / min=0.6611 avg=0.6615 max=0.6620 max-min=0.0009                        # intel
# ---------

# -D_GEMM_NAIVE_XY: reports
# ----------------------------
o_exe.-D_GEMM_NAIVE_XY.-O0. / min=2.4168 avg=2.4192 max=2.4199 max-min=0.0031                # intel
o_exe.-D_GEMM_NAIVE_XY.-O1. / min=0.6859 avg=0.6880 max=0.6903 max-min=0.0044                # intel
o_exe.-D_GEMM_NAIVE_XY.-O2. / min=0.6528 avg=0.6533 max=0.6537 max-min=0.0009                # intel
o_exe.-D_GEMM_NAIVE_XY.-O3. / min=0.6535 avg=0.6539 max=0.6543 max-min=0.0008                # intel
# ----------------------------
        
# -xHOST -mkl=sequential :-O1 mini:
45      float* xx =(float*) _mm_malloc(size_array*sizeof(float), 32);
46      float* yy =(float*) _mm_malloc(size_array*sizeof(float), 32);
58      saxpy_naive_xy(size_array, alpha+2, xx, yy);
92 void saxpy_naive_xy(int n, float alpha, float *xx, float *yy){
93         int i;
94         for (i=0; i<n; i++){
95                 yy[i]=alpha*xx[i]+yy[i];
96         }
# -O0:
# ----------------------------
no report with -O0

# -O1: not VECTORIZED
# ----------------------------
./0.sh "-D_GEMM_NAIVE_XY -O1 -qopt-report=1" && mv naive.optrpt naive.optrpt1
nothing special
./0.sh "-D_GEMM_NAIVE_XY -O1 -qopt-report=2" && mv naive.optrpt naive.optrpt2
./0.sh "-D_GEMM_NAIVE_XY -O1 -qopt-report=3" && mv naive.optrpt naive.optrpt3
./0.sh "-D_GEMM_NAIVE_XY -O1 -qopt-report=4" && mv naive.optrpt naive.optrpt4
./0.sh "-D_GEMM_NAIVE_XY -O1 -qopt-report=5" && mv naive.optrpt naive.optrpt5

# -O2: VECTORIZED
# ----------------------------
./0.sh "-D_GEMM_NAIVE_XY -O2 -qopt-report=1" && mv naive.optrpt naive.optrpt1
   -> INLINE: (58,9) saxpy_naive_xy
   LOOP BEGIN at naive.c(94,9) inlined into naive.c(58,9)
   <Peeled>
   LOOP END
   LOOP BEGIN at naive.c(94,9) inlined into naive.c(58,9)
      remark #15300: LOOP WAS VECTORIZED
   LOOP END
   LOOP BEGIN at naive.c(94,9) inlined into naive.c(58,9)
   <Remainder>
   LOOP END
# ----------------------------
./0.sh "-D_GEMM_NAIVE_XY -O2 -qopt-report=2" && mv naive.optrpt naive.optrpt2
   LOOP BEGIN at naive.c(94,9)
   <Remainder, Multiversioned v1>
      remark #15301: REMAINDER LOOP WAS VECTORIZED
   LOOP END
   LOOP BEGIN at naive.c(94,9)
   <Remainder, Multiversioned v1>
   LOOP END
   LOOP BEGIN at naive.c(94,9)
   <Multiversioned v2>
      remark #15304: loop was not vectorized: non-vectorizable loop instance from multiversioning
      remark #25439: unrolled with remainder by 2
   LOOP END
# ----------------------------
./0.sh "-D_GEMM_NAIVE_XY -O2 -qopt-report=3" && mv naive.optrpt naive.optrpt3
   LOOP BEGIN at naive.c(94,9)
   <Multiversioned v1>
   remark #25228: Loop multiversioned for Data Dependence
   remark #15300: LOOP WAS VECTORIZED
   remark #15442: entire loop may be executed in remainder
   remark #15448: unmasked aligned unit stride loads: 1
   remark #15449: unmasked aligned unit stride stores: 1
   remark #15450: unmasked unaligned unit stride loads: 1
   remark #15475: --- begin vector loop cost summary ---
   remark #15476: scalar loop cost: 13
   remark #15477: vector loop cost: 2.000
   remark #15478: estimated potential speedup: 8.510
   remark #15479: lightweight vector operations: 7
   remark #15488: --- end vector loop cost summary ---
   LOOP END
# ----------------------------
./0.sh "-D_GEMM_NAIVE_XY -O2 -qopt-report=4" && mv naive.optrpt naive.optrpt4
LOOP BEGIN at naive.c(94,9)
<Multiversioned v1>
   remark #25228: Loop multiversioned for Data Dependence
   remark #15388: vectorization support: reference yy has aligned access   [ naive.c(95,17) ]
   remark #15389: vectorization support: reference xx has unaligned access   [ naive.c(95,17) ]
   remark #15388: vectorization support: reference yy has aligned access   [ naive.c(95,17) ]
   remark #15381: vectorization support: unaligned access used inside loop body
   remark #15399: vectorization support: unroll factor set to 2
   remark #15300: LOOP WAS VECTORIZED
   remark #15442: entire loop may be executed in remainder
   remark #15448: unmasked aligned unit stride loads: 1
   remark #15449: unmasked aligned unit stride stores: 1
   remark #15450: unmasked unaligned unit stride loads: 1
   remark #15475: --- begin vector loop cost summary ---
   remark #15476: scalar loop cost: 13
   remark #15477: vector loop cost: 2.000
   remark #15478: estimated potential speedup: 8.510
   remark #15479: lightweight vector operations: 7
   remark #15488: --- end vector loop cost summary ---
LOOP END

LOOP BEGIN at naive.c(94,9)
<Remainder, Multiversioned v1>
   remark #15389: vectorization support: reference yy has unaligned access   [ naive.c(95,17) ]
   remark #15389: vectorization support: reference xx has unaligned access   [ naive.c(95,17) ]
   remark #15389: vectorization support: reference yy has unaligned access   [ naive.c(95,17) ]
   remark #15381: vectorization support: unaligned access used inside loop body
   remark #15301: REMAINDER LOOP WAS VECTORIZED
LOOP END
# ----------------------------
./0.sh "-D_GEMM_NAIVE_XY -O2 -qopt-report=5" && mv naive.optrpt naive.optrpt5


TODO ======> https://software.intel.com/node/532796
/opt/intel/composer_xe_2015/Samples/en_US/C++/vec_samples/Driver.c
/opt/intel/composer_xe_2015/Samples/en_US/Fortran/vec_samples/driver.f90
=> inteltools/readme.jg

LOOP BEGIN at naive.c(94,9)
<Multiversioned v1>
   remark #25228: Loop multiversioned for Data Dependence
   remark #15388: vectorization support: reference yy has aligned access   [ naive.c(95,17) ]
   remark #15389: vectorization support: reference xx has unaligned access   [ naive.c(95,17) ]
   remark #15388: vectorization support: reference yy has aligned access   [ naive.c(95,17) ]
   remark #15381: vectorization support: unaligned access used inside loop body
   remark #15399: vectorization support: unroll factor set to 2
   remark #15300: LOOP WAS VECTORIZED
   remark #15442: entire loop may be executed in remainder
   remark #15448: unmasked aligned unit stride loads: 1
   remark #15449: unmasked aligned unit stride stores: 1
   remark #15450: unmasked unaligned unit stride loads: 1
   remark #15475: --- begin vector loop cost summary ---
   remark #15476: scalar loop cost: 13
   remark #15477: vector loop cost: 2.000
   remark #15478: estimated potential speedup: 8.510
   remark #15479: lightweight vector operations: 7
   remark #15488: --- end vector loop cost summary ---
LOOP END

LOOP BEGIN at naive.c(94,9)
<Remainder, Multiversioned v1>
   remark #15389: vectorization support: reference yy has unaligned access   [ naive.c(95,17) ]
   remark #15389: vectorization support: reference xx has unaligned access   [ naive.c(95,17) ]
   remark #15389: vectorization support: reference yy has unaligned access   [ naive.c(95,17) ]
   remark #15381: vectorization support: unaligned access used inside loop body
   remark #15301: REMAINDER LOOP WAS VECTORIZED
LOOP END
# -------------------------------











# FLAGS="-xCORE-AVX2 "
# FLAGS="-opt-report5 -xHOST -O2 -g -mkl=sequential"
FLAGS="-qopt-report=5 -xHOST -O1 -g -mkl=sequential"
icc -D_GEMM_NAIVE_XY  $FLAGS naive.c -oAxy && mv naive.optrpt o_Axy
icc -D_GEMM_AVX2_XY   $FLAGS naive.c -oBxy && mv naive.optrpt o_Bxy
srun -n1 --exclusive ./Axy 
srun -n1 --exclusive ./Bxy
# -O2 -g:
# naive: t=0.702924 n=1000000000 x=2.000000 y=25.000000 z=0.000000
#  avx2: t=0.716473 n=1000000000 x=2.000000 y=25.000000 z=0.000000

# -O1 -g:
naive: t=0.777333 n=1000000000 x=2.000000 y=25.000000 z=0.000000
 avx2: t=0.707182 n=1000000000 x=2.000000 y=25.000000 z=0.000000 # 1.1x

	# LOOP BEGIN at naive.c(55,9)
	# <Multiversioned v1>
	#    remark #25228: Loop multiversioned for Data Dependence
	#    remark #15300: LOOP WAS VECTORIZED
	#    remark #15442: entire loop may be executed in remainder
	#    remark #15450: unmasked unaligned unit stride loads: 2
	#    remark #15451: unmasked unaligned unit stride stores: 1
	#    remark #15475: --- begin vector loop cost summary ---
	#    remark #15476: scalar loop cost: 13
	#    remark #15477: vector loop cost: 1.120
	#    remark #15478: estimated potential speedup: 7.840
	#    remark #15488: --- end vector loop cost summary ---
	# LOOP END
icc -D_GEMM_AVX2 $FLAGS naive.c -oB && mv naive.optrpt o_B
-opt-report3:
	# LOOP BEGIN at naive.c(69,9) inlined into naive.c(45,9)
	#    remark #15310: loop was not vectorized: operation cannot be vectorized   [ naive.c(72,41) ]
	#    remark #15467: unmasked aligned streaming stores: 1
	# LOOP END
	# LOOP BEGIN at naive.c(69,9)
	#    remark #15344: loop was not vectorized: vector dependence prevents vectorization. First dependence is shown below. Use level 5 report for details
	#    remark #15346: vector dependence: assumed ANTI dependence between xx line 70 and zz line 73
	# LOOP END

-opt-report5:
	# LOOP BEGIN at naive.c(69,9) inlined into naive.c(45,9)
	#    remark #15412: vectorization support: streaming store was generated for zz   [ naive.c(73,34) ]
	#    remark #15310: loop was not vectorized: operation cannot be vectorized   [ naive.c(72,41) ]
	#    remark #15467: unmasked aligned streaming stores: 1
	# LOOP END
	# LOOP BEGIN at naive.c(69,9)
	#    remark #15344: loop was not vectorized: vector dependence prevents vectorization
	#    remark #15346: vector dependence: assumed ANTI dependence between xx line 70 and zz line 73
	#    remark #15346: vector dependence: assumed FLOW dependence between zz line 73 and xx line 70
	# LOOP END


  ####    ####   ######
 #    #  #    #  #
 #       #       #####
 #       #       #
 #    #  #    #  #
  ####    ####   ######
# CCE ---------------
cd /project/csstaff/piccinal/.sph/.sph-flow.git/src_stage/CRAY/
# BRISI CN: E5-2690 v3 @ 2.60GHz Haswell / cce/8.4.0 => brisi02 !!! / cscs regression: -hcpu=haswell
# $1 -hcpu=haswell -hlist=a / n=1000000000 x=2.000000 y=5857.000000 z=0.000000
./0.sh "-D_GEMM_NAIVE_XY -O0"
./0.sh "-D_GEMM_SSE4_XY  -O0"
./0.sh "-D_GEMM_AVX2_XY  -O0"
./0.sh "-D_GEMM_NAIVE_XY -O0 -g"
./0.sh "-D_GEMM_SSE4_XY  -O0 -g"
./0.sh "-D_GEMM_AVX2_XY  -O0 -g"
# 050times:
# 250times:
#o_exe.-D_GEMM_AVX2_XY..-O0.-g. / min=0.9202 avg=0.9208 max=0.9228 max-min=0.0026 #CCE
   o_exe.-D_GEMM_AVX2_XY..-O0. / min=0.7971 avg=0.7982 max=0.7986 max-min=0.0015 #fast250cce 1.9311/0.7982=2.4x, 1.9311/0.9393=2.0x #CCE
#o_exe.-D_GEMM_NAIVE_XY.-O0.-g. / min=1.9163 avg=1.9225 max=1.9231 max-min=0.0068 #CCE
   o_exe.-D_GEMM_NAIVE_XY.-O0. / min=1.9291 avg=1.9311 max=1.9315 max-min=0.0024  #CCE
#o_exe.-D_GEMM_SSE4_XY..-O0.-g. / min=1.3079 avg=1.3099 max=1.3113 max-min=0.0034 #CCE
   o_exe.-D_GEMM_SSE4_XY..-O0. / min=0.9378 avg=0.9393 max=0.9397 max-min=0.0019  #CCE
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O1"           
./0.sh "-D_GEMM_SSE4_XY  -O1"           
./0.sh "-D_GEMM_AVX2_XY  -O1"           
./0.sh "-D_GEMM_NAIVE_XY -O1 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O1 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O1 -g"        
# 250times:
#o_exe.-D_GEMM_AVX2_XY..-O1.-g. / min=0.9190 avg=0.9198 max=0.9219 max-min=0.0029        #CCE
   o_exe.-D_GEMM_AVX2_XY..-O1. / min=0.6518 avg=0.6524 max=0.6528 max-min=0.001          #CCE
#o_exe.-D_GEMM_NAIVE_XY.-O1.-g. / min=1.9154 avg=1.9226 max=1.9237 max-min=0.0083        #CCE
   o_exe.-D_GEMM_NAIVE_XY.-O1. / min=0.7260 avg=0.7275 max=0.7290 max-min=0.003          #CCE
#o_exe.-D_GEMM_SSE4_XY..-O1.-g. / min=1.3045 avg=1.3059 max=1.3092 max-min=0.0047        #CCE
   o_exe.-D_GEMM_SSE4_XY..-O1. / min=0.6260 avg=0.6269 max=0.6282 max-min=0.0022 #fast250cce but similar timings #CCE
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O2"           
./0.sh "-D_GEMM_SSE4_XY  -O2"           
./0.sh "-D_GEMM_AVX2_XY  -O2"           
./0.sh "-D_GEMM_NAIVE_XY -O2 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O2 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O2 -g"        
# 250times: for i in o_exe.*O2*;do ./res.sh $i ;done
#o_exe.-D_GEMM_AVX2_XY..-O2.-g. / min=0.9145 avg=0.9155 max=0.9174 max-min=0.0029       #CCE
   o_exe.-D_GEMM_AVX2_XY..-O2. / min=0.6577 avg=0.6582 max=0.6592 max-min=0.0015        #CCE
#o_exe.-D_GEMM_NAIVE_XY.-O2.-g. / min=1.9125 avg=1.9162 max=1.9166 max-min=0.0041       #CCE
   o_exe.-D_GEMM_NAIVE_XY.-O2. / min=0.6622 avg=0.6626 max=0.6656 max-min=0.0034        #CCE
#o_exe.-D_GEMM_SSE4_XY..-O2.-g. / min=1.3126 avg=1.3143 max=1.3147 max-min=0.0021       #CCE
   o_exe.-D_GEMM_SSE4_XY..-O2. / min=0.6335 avg=0.6340 max=0.6345 max-min=0.001  #fast250cce but similar timings #CCE
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O3"           
./0.sh "-D_GEMM_SSE4_XY  -O3"           
./0.sh "-D_GEMM_AVX2_XY  -O3"           
./0.sh "-D_GEMM_NAIVE_XY -O3 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O3 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O3 -g"        
# 250times: for i in o_exe.*O3*;do ./res.sh $i ;done
#o_exe.-D_GEMM_AVX2_XY..-O3.-g. / min=0.9166 avg=0.9172 max=0.9187 max-min=0.0021       #CCE
   o_exe.-D_GEMM_AVX2_XY..-O3. / min=0.6566 avg=0.6572 max=0.6577 max-min=0.0011        #CCE
#o_exe.-D_GEMM_NAIVE_XY.-O3.-g. / min=1.9150 avg=1.9228 max=1.9240 max-min=0.009        #CCE
   o_exe.-D_GEMM_NAIVE_XY.-O3. / min=0.6642 avg=0.6645 max=0.6647 max-min=0.0005        #CCE
#o_exe.-D_GEMM_SSE4_XY..-O3.-g. / min=1.3089 avg=1.3107 max=1.3110 max-min=0.0021       #CCE
   o_exe.-D_GEMM_SSE4_XY..-O3. / min=0.6349 avg=0.6356 max=0.6361 max-min=0.0012 #fast250cce but similar timings #CCE


# -D_GEMM_NAIVE_XY: reports
# ----------------------------
45      float* xx =(float*) _mm_malloc(size_array*sizeof(float), 32);
46      float* yy =(float*) _mm_malloc(size_array*sizeof(float), 32);
55         for (i=0; i<250; i++){
58      saxpy_naive_xy(size_array, alpha+2, xx, yy);
92 void saxpy_naive_xy(int n, float alpha, float *xx, float *yy){
93         int i;
94         for (i=0; i<n; i++){
95                 yy[i]=alpha*xx[i]+yy[i];
96         }
# ----------------------------
   o_exe.-D_GEMM_NAIVE_XY.-O0. / min=1.9291 avg=1.9311 max=1.9315 max-min=0.0024
   o_exe.-D_GEMM_NAIVE_XY.-O1. / min=0.7260 avg=0.7275 max=0.7290 max-min=0.003
   o_exe.-D_GEMM_NAIVE_XY.-O2. / min=0.6622 avg=0.6626 max=0.6656 max-min=0.0034
   o_exe.-D_GEMM_NAIVE_XY.-O3. / min=0.6642 avg=0.6645 max=0.6647 max-min=0.0005
# ----------------------------
# -O1: 
./0.sh "-D_GEMM_NAIVE_XY -O0 -hlist=a" && mv naive.lst naive.lst-O0
./0.sh "-D_GEMM_NAIVE_XY -O1 -hlist=a" && mv naive.lst naive.lst-O1
CC-6287 CC: VECTOR File = naive.c, Line = 55
  A loop was not vectorized because it contains a call to function "mysecond" on line 57.
  if i remove calls to mysecond then i get:
  A loop was not vectorized because it contains a call to function "saxpy_naive_xy" on line 58.
  => try omp timer ?

CC-6005 CC: SCALAR File = naive.c, Line = 94
  A loop was unrolled 2 times.

CC-6213 CC: VECTOR File = naive.c, Line = 94
  A loop was conditionally vectorized.

#mysecond: CC-3021 CC: IPA File = naive.c, Line = 159
#mysecond:   "gettimeofday" (called from "mysecond") was not inlined because the compiler was unable to locate the routine.
./0.sh "-D_GEMM_NAIVE_XY -O2 -hlist=a" && mv naive.lst naive.lst-O2
~idem
./0.sh "-D_GEMM_NAIVE_XY -O3 -hlist=a" && mv naive.lst naive.lst-O3
~idem




# Compile with -rm/-hlist => .lst
#FLAGS="-hcpu=haswell -hlist=a -O0"
#FLAGS="-hcpu=haswell -hlist=a -O1"
FLAGS="-hcpu=haswell -hlist=a -O2 -g"
#FLAGS="-hcpu=haswell -hlist=a -O3"
cc -D_GEMM_NAIVE_XY  $FLAGS naive.c -oAxy && mv naive.lst o_Axy
cc -D_GEMM_AVX2_XY   $FLAGS naive.c -oBxy && mv naive.lst o_Bxy
# cc -D_GEMM_NAIVE_XYZ $FLAGS naive.c -oAxyz && mv naive.lst o_Axyz
# cc -D_GEMM_AVX2_XYZ  $FLAGS naive.c -oBxyz && mv naive.lst o_Bxyz
#
time -p srun -n1 --exclusive ./Axy  # naive: n=1000000000 x=2.000000 y=25.000000 z=0.000000 real 12.49
time -p srun -n1 --exclusive ./Bxy  #  avx2: n=1000000000 x=2.000000 y=25.000000 z=0.000000 real 10.68
# time -p srun -n1 --exclusive ./Axyz # naive: n=1000000000 x=2.000000 y=1.000000 z=25.000000 real 7.52
# time -p srun -n1 --exclusive ./Bxyz #  avx2: n=1000000000 x=2.000000 y=1.000000 z=25.000000 real 6.54

### PTL/6.3.0 (--- CCE only ---): 
||  29.6% | 2.185928 |   -- |    -- |   1.0 |saxpy_naive_xy
||  15.4% | 0.955958 |   -- |    -- |   1.0 |saxpy_avx2_xy      2.3x
#||  32.3% | 2.538502 |   -- |    -- |   1.0 |saxpy_naive_xyz
#||  21.0% | 1.390272 |   -- |    -- |   1.0 |saxpy_avx2_xyz     1.8x

### internal timer:
time -p srun -n1 --exclusive ./Axy # naive: t=2.008385 n=1000000000 x=2.000000 y=25.000000 z=0.000000 real 8.90
time -p srun -n1 --exclusive ./Bxy #  avx2: t=0.955115 n=1000000000 x=2.000000 y=25.000000 z=0.000000 real 7.54
# => 2.1x

#  ------------------
#   Loop stats can be used in the loop_info compiler directives:
#    !DIR$        LOOP_INFO est_trips(Avg) min_trips(Min) max_trips(Max)
#    #pragma _CRI loop_info est_trips(Avg) min_trips(Min) max_trips(Max)
#   The compiler interprets Avg as an estimate, but Min and Max as guarantees.

# FLAGS="-hcpu=haswell -hlist=a -O2":
# naive: n=1000000000 x=2.000000 y=1.000000 z=25.000000 real 15.45
        # CC-6005 CC: SCALAR File = naive.c, Line = 55 A loop was unrolled 2 times.
        # CC-6213 CC: VECTOR File = naive.c, Line = 55 A loop was conditionally vectorized.
# avx2:  n=1000000000 x=2.000000 y=1.000000 z=25.000000 real 3.94
	# CC-6317 CC: VECTOR File = naive.c, Line = 69
	# A loop was not vectorized because it contains inline intrinsic operations at
	# line 72 corresponding to explicit SIMD parallelism
	# CC-3182 CC: IPA File = naive.c, Line = 69
	# Loop has been flattened.




 #####    ####      #
 #    #  #    #     #
 #    #  #          #
 #####   #  ###     #
 #       #    #     #
 #        ####      #
# PGI -----------------
cd /project/csstaff/piccinal/.sph/.sph-flow.git/src_stage/PGI/
# BRISI CN: E5-2690 v3 @ 2.60GHz Haswell / pgi/15.7.0 => brisi02 !!! / cscs regression: -tp=haswell
# $1 -Mvect -tp=haswell -Mneginfo -Minfo / n=1000000000 x=2.000000 y=5857.000000 z=0.000000
# pgcc-Info-Switch -Mvect forces -O2
./0.sh "-D_GEMM_NAIVE_XY -O0"
./0.sh "-D_GEMM_SSE4_XY  -O0"
./0.sh "-D_GEMM_AVX2_XY  -O0"
./0.sh "-D_GEMM_NAIVE_XY -O0 -g"
./0.sh "-D_GEMM_SSE4_XY  -O0 -g"
./0.sh "-D_GEMM_AVX2_XY  -O0 -g"
# 050times:
# 250times:
#o_exe.-D_GEMM_AVX2_XY..-O0.-g. / min=1.8665 avg=1.8675 max=1.8677 max-min=0.0012               #pgi
   o_exe.-D_GEMM_AVX2_XY..-O0. / min=1.3524 avg=1.3530 max=1.3533 max-min=0.0009                #pgi
#o_exe.-D_GEMM_NAIVE_XY.-O0.-g. / min=2.3244 avg=2.3249 max=2.3253 max-min=0.0009               #pgi
   o_exe.-D_GEMM_NAIVE_XY.-O0. / min=0.7184 avg=0.7197 max=0.7446 max-min=0.0262 #fast250pgi    #pgi
#o_exe.-D_GEMM_SSE4_XY..-O0.-g. / min=3.1051 avg=3.1056 max=3.1076 max-min=0.0025               #pgi
   o_exe.-D_GEMM_SSE4_XY..-O0. / min=2.2537 avg=2.2565 max=2.2590 max-min=0.0053                #pgi
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O1 "           
./0.sh "-D_GEMM_SSE4_XY  -O1 "           
./0.sh "-D_GEMM_AVX2_XY  -O1 "           
./0.sh "-D_GEMM_NAIVE_XY -O1 -g"
./0.sh "-D_GEMM_SSE4_XY  -O1 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O1 -g"        
# 250times:
#o_exe.-D_GEMM_AVX2_XY..-O1.-g. / min=1.1494 avg=1.1510 max=1.1557 max-min=0.0063                #pgi
   o_exe.-D_GEMM_AVX2_XY..-O1. / min=0.6723 avg=0.6726 max=0.6733 max-min=0.001    #fast250pgi   #pgi
#o_exe.-D_GEMM_NAIVE_XY.-O1.-g. / min=1.0891 avg=1.0939 max=1.1535 max-min=0.0644                #pgi
  o_exe.-D_GEMM_NAIVE_XY.-O1.. / min=0.7183 avg=0.7214 max=0.7964 max-min=0.0781                 #pgi
#o_exe.-D_GEMM_SSE4_XY..-O1.-g. / min=1.8929 avg=1.8936 max=1.8942 max-min=0.0013                #pgi
  o_exe.-D_GEMM_SSE4_XY..-O1.. / min=1.8641 avg=1.8666 max=1.8670 max-min=0.0029                 #pgi
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O2 -Mvect"           
./0.sh "-D_GEMM_SSE4_XY  -O2 -Mvect"           
./0.sh "-D_GEMM_AVX2_XY  -O2 -Mvect"           
./0.sh "-D_GEMM_NAIVE_XY -O2 -g -Mvect"        
./0.sh "-D_GEMM_SSE4_XY  -O2 -g -Mvect"        
./0.sh "-D_GEMM_AVX2_XY  -O2 -g -Mvect"        
# 250times: for i in o_exe.*O2*;do ./res.sh $i ;done
o_exe.-D_GEMM_AVX2_XY..-O2.-Mvect. / min=0.6710 avg=0.6715 max=0.6717 max-min=0.0007                             #pgi
#o_exe.-D_GEMM_AVX2_XY..-O2.-g.-Mvect. / min=0.6699 avg=0.6703 max=0.6732 max-min=0.0033                         #pgi
o_exe.-D_GEMM_NAIVE_XY.-O2.-Mvect. / min=0.6637 avg=0.6641 max=0.6704 max-min=0.0067      #fast250pgi but ...    #pgi
#o_exe.-D_GEMM_NAIVE_XY.-O2.-g.-Mvect. / min=0.6655 avg=0.6658 max=0.6662 max-min=0.0007                         #pgi
o_exe.-D_GEMM_SSE4_XY..-O2.-Mvect. / min=0.6707 avg=0.6712 max=0.6715 max-min=0.0008                             #pgi
#o_exe.-D_GEMM_SSE4_XY..-O2.-g.-Mvect. / min=0.6768 avg=0.6776 max=0.6779 max-min=0.0011                         #pgi
# ---------
./0.sh "-D_GEMM_NAIVE_XY -O3"           
./0.sh "-D_GEMM_SSE4_XY  -O3"           
./0.sh "-D_GEMM_AVX2_XY  -O3"           
./0.sh "-D_GEMM_NAIVE_XY -O3 -g"        
./0.sh "-D_GEMM_SSE4_XY  -O3 -g"        
./0.sh "-D_GEMM_AVX2_XY  -O3 -g"        
# 250times: for i in o_exe.*O3*;do ./res.sh $i ;done
#o_exe.-D_GEMM_AVX2_XY..-O3.-g. / min=0.6607 avg=0.6612 max=0.6617 max-min=0.001                       #pgi
   o_exe.-D_GEMM_AVX2_XY..-O3. / min=0.6717 avg=0.6720 max=0.6726 max-min=0.0009                       #pgi
#o_exe.-D_GEMM_NAIVE_XY.-O3.-g. / min=0.6674 avg=0.6679 max=0.6750 max-min=0.0076                      #pgi
   o_exe.-D_GEMM_NAIVE_XY.-O3. / min=0.6571 avg=0.6573 max=0.6576 max-min=0.0005   #fast250pgi but ... #pgi
#o_exe.-D_GEMM_SSE4_XY..-O3.-g. / min=0.6753 avg=0.6760 max=0.6783 max-min=0.003                       #pgi
   o_exe.-D_GEMM_SSE4_XY..-O3. / min=0.6707 avg=0.6710 max=0.6716 max-min=0.0009                       #pgi
# ---------

# -D_GEMM_NAIVE_XY: reports
# ----------------------------
45      float* xx =(float*) _mm_malloc(size_array*sizeof(float), 32);
46      float* yy =(float*) _mm_malloc(size_array*sizeof(float), 32);
55         for (i=0; i<250; i++){
58      saxpy_naive_xy(size_array, alpha+2, xx, yy);
92 void saxpy_naive_xy(int n, float alpha, float *xx, float *yy){
93         int i;
94         for (i=0; i<n; i++){
95                 yy[i]=alpha*xx[i]+yy[i];
96         }
# ----------------------------
   o_exe.-D_GEMM_NAIVE_XY.-O0.     / min=0.7184 avg=0.7197 max=0.7446 max-min=0.0262 
  o_exe.-D_GEMM_NAIVE_XY.-O1..     / min=0.7183 avg=0.7214 max=0.7964 max-min=0.0781
o_exe.-D_GEMM_NAIVE_XY.-O2.-Mvect. / min=0.6637 avg=0.6641 max=0.6704 max-min=0.0067
   o_exe.-D_GEMM_NAIVE_XY.-O3.     / min=0.6571 avg=0.6573 max=0.6576 max-min=0.0005
# ----------------------------
# -O1: (-Mvect adds -O2)
./0.sh "-D_GEMM_NAIVE_XY -O1 -Mneginfo -Minfo" &> naive.optrptO1
no report

# -O2:
./0.sh "-D_GEMM_NAIVE_XY -O2 -Mneginfo -Minfo" &> naive.optrptO2
     55, Loop not vectorized/parallelized: contains call
saxpy_naive_xy:
     94, Generated 3 alternate versions of the loop
         Generated vector and scalar versions of the loop; pointer conflict tests determine which is executed
         Generated 2 prefetch instructions for the loop
         Loop unrolled 16 times
         Generated 1 prefetches in scalar loop

# -O3:
./0.sh "-D_GEMM_NAIVE_XY -O3 -Mneginfo -Minfo" &> naive.optrptO3
idem -O2



FLAGS="-Mneginfo=all -Mvect -fast"
FLAGS="-O2 -g -Mneginfo=vect -Minfo=vect -Mvect -fast"
FLAGS="-O1 -g -Mneginfo=vect -Minfo=vect -Mvect -fast"
pgcc -D_GEMM_NAIVE_XY $FLAGS naive.c -oAxy &> o_Axy
pgcc -D_GEMM_AVX2_XY  $FLAGS naive.c -oBxy &> o_Bxy
#
~/sbatch.sh brisi 1 Axy 1 1 1 #
~/sbatch.sh brisi 1 Bxy 1 1 1 # 
#
o_Axy.0001.1.1.1.brisi.:naive: t=0.662471 n=1000000000 x=2.000000 y=25.000000 z=0.000000
o_Bxy.0001.1.1.1.brisi.: avx2: t=0.670892 n=1000000000 x=2.000000 y=25.000000 z=0.000000

-Minfo=all      -Mneginfo=all   -Mvect
-Minfo=vect     -Mneginfo=vect  -Mvect
-Mvect=simd:[128|256] -fma -Minline
-fast
# => stdout => .pgiinfo





  ##    #    #  #    #
 #  #   #    #   #  #
#    #  #    #    ##
######  #    #    ##
#    #   #  #    #  #
#    #    ##    #    #
## avx2:
CFLAGS="-O3 -openmp -g -mavx" # -msse4.2
icc -c $CFLAGS dgemm.c
icc -c $CFLAGS dgemm-avx2.c
icc dgemm.o dgemm-avx2.o $CFLAGS -mkl=sequential 
mv a.out avx2.mavx
# 
time -p ./avx2.mavx 2048  # Gflop/s: 33.9917 real 0.58
time -p ./avx2.sse42 2048 # Gflop/s: 36.1965 real 0.56












  ##    #    #  #    #   ####    #####    ##     #####  ######
 #  #   ##   #  ##   #  #    #     #     #  #      #    #
#    #  # #  #  # #  #  #    #     #    #    #     #    #####
######  #  # #  #  # #  #    #     #    ######     #    #
#    #  #   ##  #   ##  #    #     #    #    #     #    #
#    #  #    #  #    #   ####      #    #    #     #    ######
# -------------- advisor_annotate:F90 --------------

#ifdef _advisor_annotate
#include "advisor-annotate.h"
#endif

#ifdef _advisor_annotate
  ANNOTATE_SITE_BEGIN(solve);
#endif

  for(int i=0; i<size; i++) {
#ifdef _advisor_annotate
    ANNOTATE_ITERATION_TASK(setQueen);
#endif
    ...
  }

#ifdef _advisor_annotate
  ANNOTATE_SITE_END();
#endif
# -------------- advisor_annotate:F90 --------------
 

# -------------- advisor_annotate:C --------------
#ifdef _advisor_annotate
use advisor_annotate
#endif

#ifdef _advisor_annotate
call annotate_site_begin("solve")
#endif

do i=1, ...
#ifdef _advisor_annotate
call annotate_iteration_task("setQueen")
#endif
enddo

#ifdef _advisor_annotate
call annotate_site_end()
#endif
# -------------- advisor_annotate --------------


# -------------- advisor_annotate --------------
# Compile with:
1. Set up your command line environment by sourcing this script file: source /opt/intel/advisor_xe_2016/advixe-vars.sh
2. Add this compiler option to your build command: -I${ADVISOR_XE_2016_DIR}/include
3. Add this linker option to your build command: -ldl

\#include "advisor-annotate.h"  #// Add to each module that contains Intel Advisor annotations
ANNOTATE_SITE_BEGIN( MySite1 );  #// Place before the loop control statement (and before any loop directives) to begin a parallel code region (parallel site).
#// loop control statement
    ANNOTATE_ITERATION_TASK( MyTask1 );  #// Place at the start of loop body. This annotation identifies an entire body as a task. 
    #// loop body
ANNOTATE_SITE_END();  // End the parallel code region, after task execution completes
# -------------- advisor_annotate --------------



 ####   #    #  #          #    #    #  ######  #####    ####    ####
#    #  ##   #  #          #    ##   #  #       #    #  #    #  #    #
#    #  # #  #  #          #    # #  #  #####   #    #  #    #  #
#    #  #  # #  #          #    #  # #  #       #    #  #    #  #
#    #  #   ##  #          #    #   ##  #       #    #  #    #  #    #
 ####   #    #  ######     #    #    #  ######  #####    ####    ####
# annotation help => /usr/bin/xdg-open: line 365: exo-open: command not found => needs html browser...
# => export PATH=/apps/common/UES/sandbox/jgp/exoopen/bin:$PATH
cat /apps/common/UES/sandbox/jgp/exoopen/bin/exo-open 
        # #!/bin/bash
        # /apps/common/UES/sandbox/jgp/firefox/42.0/firefox -no-remote "$@"





















 ####   #       #####
#    #  #       #    #
#    #  #       #    #
#    #  #       #    #
#    #  #       #    #
 ####   ######  #####
### Scicomp:
make CXXLINK="scorep --cuda CC"
        CC  -Wall -g -O3 -fopenmp -DNDEBUG -D_GPU_ -c -o ./main.o main.cpp
        scorep --cuda nvcc -arch=sm_35 -m64 -c -o ./cuda_interface.o cuda_interface.cu
        scorep --cuda CC -o test.x ./main.o cuda_interface.o -fopenmp  -lcublas -lcufft -lcudart

make CXX="scorep --openmp CC" CXXLINK="scorep --cuda CC"
        scorep --openmp CC -Wall -g -O3 -fopenmp -DNDEBUG -D_GPU_ -c -o ./main.o main.cpp
        scorep --cuda nvcc -arch=sm_35 -m64 -c -o ./cuda_interface.o cuda_interface.cu
        scorep --cuda CC -o test.x ./main.o cuda_interface.o -fopenmp  -lcublas -lcufft -lcudart

make CXX="scorep --user CC" CXXLINK="scorep --user CC" NVCC="scorep --user nvcc"


make CXX="scorep --openmp CC" CXXLINK="scorep --openmp --cuda CC" NVCC="scorep --cuda nvcc" 

1/ TRACE=false
scorep-score scorep-20140129_1316_782794813431197/profile.cubex 

Estimated aggregate size of event trace:                   9'530'784'658 bytes
Estimated requirements for largest trace buffer (max_tbc): 9'530'784'658 bytes
(hint: When tracing set SCOREP_TOTAL_MEMORY > max_tbc to avoid intermediate flushes
 or reduce requirements using file listing names of USR regions to be filtered.)

flt type         max_tbc         time      % region
     ALL      9530784658        92.48  100.0 ALL
     USR      9530784570        53.19   57.5 USR
     COM              44        28.76   31.1 COM
     OMP              44        10.52   11.4 OMP



scorep-score -r scorep-20140129_1316_782794813431197/profile.cubex 

Estimated aggregate size of event trace:                   9530784658 bytes
Estimated requirements for largest trace buffer (max_tbc): 9530784658 bytes
(hint: When tracing set SCOREP_TOTAL_MEMORY > max_tbc to avoid intermediate flushes
 or reduce requirements using file listing names of USR regions to be filtered.)

flt type         max_tbc         time      % region
     ALL      9530784658        92.48  100.0 ALL
     USR      9530784570        53.19   57.5 USR
     COM              44        28.76   31.1 COM
     OMP              44        10.52   11.4 OMP
     # -------------------------------------------- #   
     USR      2419560000        10.57   11.4 complex_mult(double*, double*, double*)
     USR      2200786896        10.17   11.0 mdarray<double, 2>::operator()(int, int)
     USR      1672821744         7.98    8.6 mdarray<std::complex<double>, 2>::operator()(int, int)
     USR      1613040066         7.57    8.2 std::complex<double>::complex(double, double)
     USR       366080000         6.51    7.0 std::complex<double> std::exp<double>(std::complex<double> const&)
     USR       366080000         1.74    1.9 std::complex<double>::__rep() const
     USR       366080000         2.47    2.7 std::__complex_exp(doublecomplex )
     USR       366080000         1.63    1.8 std::complex<double>::complex(doublecomplex )
     USR        59558400         0.26    0.3 std::complex<double>::imag() const
     USR        59558400         0.26    0.3 std::complex<double>::real() const
     USR        40608000         0.78    0.8 std::complex<double>& std::complex<double>::operator+=<double>(std::complex<double> const&)
     USR          297792         0.00    0.0 mdarray<int, 2>::operator()(int, int)
     USR          183128         0.00    0.0 mdarray<int, 1>::operator()(int)
     USR            5984         0.00    0.0 std::vector<int, std::allocator<int> >::operator[](unsigned long)
     USR            1760         0.00    0.0 std::vector<dimension, std::allocator<dimension> >::size() const
     USR            1760         0.00    0.0 __gnu_cxx::__normal_iterator<dimension*, std::vector<dimension, std::allocator<dimension> > >::base() const
     USR            1584         0.00    0.0 std::_Vector_base<dimension, std::allocator<dimension> >::_M_get_Tp_allocator()
     USR            1320         0.00    0.0 __gnu_cxx::new_allocator<dimension>::max_size() const
     USR             990         0.00    0.0 dimension::size()


2/ cat scorep.filter 
        SCOREP_REGION_NAMES_BEGIN EXCLUDE
        complex_mult*
        mdarray*
        std::complex*

scorep-score -f scorep.filter scorep-20140129_1316_782794813431197/profile.cubex 
        Estimated aggregate size of event trace:                   366'126'184 bytes


3/ TRACE=true
scorep-score -r scorep-20140129_1324_783283331592110/profile.cubex 

Estimated aggregate size of event trace:                   366126184 bytes
Estimated requirements for largest trace buffer (max_tbc): 366126184 bytes
(hint: When tracing set SCOREP_TOTAL_MEMORY > max_tbc to avoid intermediate flushes
 or reduce requirements using file listing names of USR regions to be filtered.)

flt type         max_tbc         time      % region
     ALL       366126184        15.66  100.0 ALL
     USR       366126096         6.27   40.0 USR
     COM              44         7.84   50.1 COM
     OMP              44         1.55    9.9 OMP

     USR       366080000         3.03   19.4 std::__complex_exp(doublecomplex )
     USR            5984         0.00    0.0 std::vector<int, std::allocator<int> >::operator[](unsigned long)
     USR            1760         0.00    0.0 std::vector<dimension, std::allocator<dimension> >::size() const


# ------------------ VAMPIR
make CXX="vtCC -vt:hyb" CXXLINK="vtCC -vt:hyb" NVCC="vtnvcc"
        vtCC -vt:hyb -g -O3 -fopenmp -DNDEBUG -D_GPU_ -c -o ./main.o main.cpp
        vtnvcc -arch=sm_35 -m64 -c -o ./cuda_interface.o cuda_interface.cu
        vtCC -vt:hyb -o test.x ./main.o cuda_interface.o -fopenmp  -lcublas -lcufft -lcudart



        VT_MAX_FLUSHES=10
        VT_BUFFER_SIZE=2000M
        VT_GPUTRACE=no
        VT_MODE=STAT

# ------------------ VAMPIR
make CXX="vtCC -vt:hyb" CXXLINK="vtCC -vt:hyb" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_"


Add mpi_init/mpi_fin
make CXX="vtCC -vt:hyb" CXXLINK="vtCC -vt:hyb" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_"
export VT_MODE=STAT
export OMP_NUM_THREADS=1 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./test.x

./test.x.prof.txt
                                    excl. time  incl. time
*excl. time  incl. time      calls      / call      / call  name
   13.606s     45.801s           1    13.606s     45.801s   main
    5.481s      5.481s   109980000    49.614ns    49.614ns  complex_mult(double*, double*, double*)
    5.093s     14.409s           1     5.093s     14.409s   generate_beta_phi_gpu_v1(int, int, int, mdarray<int, 2>&, mdarray<std::complex<double>, 2>&, mdarra
y<std::complex<double>, 2>&, mdarray<std::complex<double>, 2>&, mdarray<std::complex<double>, 2>&)
    4.630s      4.630s   100035768    46.152ns    46.152ns  mdarray<double, 2>::operator()(int, int)
    3.463s      3.463s    76037352    45.383ns    45.383ns  mdarray<std::complex<double>, 2>::operator()(int, int)
    3.430s      3.430s    73320003    46.537ns    46.537ns  std::complex<double>::complex(double, double)
    3.170s      6.172s    16640000     0.190us     0.371us  std::complex<double> std::exp<double>(std::complex<double> const&)
    1.556s      1.556s    16640000    93.458ns    93.458ns  std::__complex_exp(doublecomplex )
    1.303s      1.303s           1     1.303s      1.303s   cuda_copy_to_host
    0.723s      0.723s    16640000    43.075ns    43.075ns  std::complex<double>::__rep() const

main;generate_beta_phi_gpu_v1;

######  #######    #    ######  #     # #######
#     # #         # #   #     # ##   ## #
#     # #        #   #  #     # # # # # #
######  #####   #     # #     # #  #  # #####
#   #   #       ####### #     # #     # #
#    #  #       #     # #     # #     # #
#     # ####### #     # ######  #     # #######
        https://portal.futuregrid.org/manual/vampir/trace



# ------------------ VAMPIR
make CXX="vtCC -vt:inst manual -DVTRACE" CXXLINK="vtCC -vt:inst manual -DVTRACE" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_ -D__CSCSTRACE"  
export OMP_NUM_THREADS=1 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./test.x 
        VampirTrace: FATAL: vt_fork.c:61: Assertion trcid_filename[0] != '\0' failed
        Please report this incident to vampirsupport@zih.tu-dresden.de
        [unset]:_pmi_daemon_barrier:daemon pipe read failed from PE 0 errno = Success
        [unset]:_pmiu_daemon:_pmi_daemon_barrier failed 




# ------------------ VAMPIR
mm CC="vtcc -vt:inst manual -DVTRACE -D__CSCS_TRACE" # NVCC="vtnvcc"






# ------------------ VAMPIR (~OK)
## santis/daint

export VT_MAX_FLUSHES=1
# export VT_GPUTRACE=cupti,runtime,kernel,memcpy,concurrent # and others
export VT_GPUTRACE=yes
export VT_BUFFER_SIZE=8G                                  # Just big enough
export VT_CUDATRACE_BUFFER_SIZE=20000000
export MPICH_G2G_PIPELINE=27
export MPICH_RDMA_ENABLED_CUDA=1

#eff make CXX="vtCC -vt:mpi -vt:inst manual -DVTRACE" CXXLINK="vtCC -vt:mpi -vt:inst manual" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_"
#eff salloc -N1
#eff export OMP_NUM_THREADS=1 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./test.x
#eff 
#eff # -DVTRACE ?
#eff make CXX="vtCC -vt:hyb -vt:inst manual " CXXLINK="vtCC -vt:hyb -vt:inst manual" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_ -fopenmp"
#eff # openmp = cpu = generate_beta_phi_cpu
#eff export OMP_NUM_THREADS=8 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./test.x
#eff 



# GPU1 -----------------------------------------------------------------
mmc; make CXX="vtCC -vt:mpi -vt:inst manual " CXXLINK="vtCC -vt:mpi -vt:inst manual" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_ -D__CSCS_GPU1 "
mv test.x GPU1
export ...
export OMP_NUM_THREADS=1 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./GPU1
vampir GPU1.otf



# GPU2 -----------------------------------------------------------------
mmc; make CXX="vtCC -vt:mpi -vt:inst manual " CXXLINK="vtCC -vt:mpi -vt:inst manual" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_ -D__CSCS_GPU2 "
mv test.x GPU2
export OMP_NUM_THREADS=1 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./GPU2
vampir GPU2.otf



# GPU3 -----------------------------------------------------------------
mmc; make CXX="vtCC -vt:mpi -vt:inst manual " CXXLINK="vtCC -vt:mpi -vt:inst manual" NVCC="vtnvcc" CXX_OPT="-g -O3 -DNDEBUG -D_GPU_ -D__CSCS_GPU3 "
mv test.x GPU3
export OMP_NUM_THREADS=1 ; aprun -n1 -N1 -d$OMP_NUM_THREADS ./GPU3
vampir GPU3.otf









