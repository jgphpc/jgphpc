{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to MPI\n",
    "\n",
    "<div class=\"dateauthor\">\n",
    "11 October 2018 | Jan H. Meinke\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "MPI (Message Passing Interface) is the most used protocol for communicating between processes.  It doesn't matter if the processes that want to talk to each other are on the same or different nodes (i.e., computers). In this tutorial, we'll use `mpi4py` to learn about MPI and its API.\n",
    "\n",
    "An MPI program can run on a single computer/node with one or more processes that share memory or on multiple computers that are connected to each other via some network (not shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Six nodes](images/6nodes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Communicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "MPI processes talk to each other via communicators. The global communicator `MPI.COMM_WORLD` connects all MPI processes. Each process has a unique *rank* with `MPI.COMM_WORLD`. The process with `rank=0` is usually called the root process. MPI processes can participate in different communicators and may have a different rank in these communicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Communicators](images/6nodeswcommranks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point to point communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Sending messages from one MPI task to another is the foundation of MPI. Messages consist of some meta information such as the *source* and the *destination*, a *tag* that identifies the message, the *data type*, and the *count* of data items and the actual data.\n",
    "\n",
    "For each `Send` there has to be a matching `Recv`. This means that the meta information has to fit (including the tag). There are some wildcards and some additional commands that make this more flexible.\n",
    "\n",
    "Sending and receiving can be blocking or non-blocking. In the latter case, the flow of the program continuous after the call. In the former the program waits until the message has been transmitted. There is a very real danger for deadlocks here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Point to point communication](images/6nodesptp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Collective calls are performed by all ranks of a communicator and thus *must* be called by all ranks. Examples for collective calls are `Bcast`, `Scatter`, `Gather`, and `Allreduce`. We'll look at some collective calls later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Collective Call](images/6nodescoll1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You could implement collective calls yourself using `Send` and `Recv`, but using the provided collective calls makes your code easier to read and allows vendors to optimize MPI for their platform. For example, in the picture above all the calls share the bandwidth of rank 0. One could use a tree-like pattern to balance the network load instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Collective Call Tree](images/6nodescoll2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You can find more information about MPI at\n",
    "\n",
    "* http://materials.jeremybejarano.com/MPIwithPython/index.html\n",
    "\n",
    "And the documentation of mpi4py at http://mpi4py.scipy.org/docs/usrman/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting the engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can combine ipyparalle and MPI. To do that, we need to start some IPython engines. For this notebook, we'll use some local engines. The easiest way to start them is by typing ``ipcluster start --engines=MPIEngineSetLauncher`` into a terminal window. This will start as many engines as there are virtual processors on your machine.\n",
    "\n",
    "Please, go ahead and start the engines before you proceed. Make sure you have the right environment loaded (`source hpcpy18`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Connecting to the engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, we want to connect to the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipyparallel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b8648c4bc283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mipyparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#rc.ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipyparallel'"
     ]
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, we need to create a view and activate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "view = rc[:]\n",
    "view.activate()\n",
    "view.block = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up for MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We are now ready to use MPI with our IPython notebook.\n",
    "\n",
    "We will use the cell magic ``%%px`` and the line magic ``%px`` to execute commands on all the engines. So, whenenver you see ``%%px`` the cell is executed on *all* the engines, but not in the process that controls your notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%px from mpi4py import MPI # Import and initialize MPI on the engines\n",
    "from mpi4py import MPI # Import and initialize MPI in the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%px import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Importing MPI initializes the MPI and sets up the default communicator `COMM_WORLD`, which includes all processes involved in this MPI program.\n",
    "\n",
    "Using `COMM_WORLD`, each process can determine its *rank* and the total number of ranks available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] I'm 1 of 1. Resistance is futile.\n",
      "[stdout:1] I'm 1 of 1. Resistance is futile.\n",
      "[stdout:2] I'm 1 of 1. Resistance is futile.\n",
      "[stdout:3] I'm 1 of 1. Resistance is futile.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(\"I'm %d of %d. Resistance is futile.\" % (rank + 1, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point to point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can send messages from one rank to another. \n",
    "\n",
    "The following exercises will only work if more than one process is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import sys\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "a = np.ones(1)\n",
    "b = np.zeros(1)\n",
    "if (size < 2):\n",
    "    print (\"Warning! Not enough ranks available!\" )\n",
    "else:\n",
    "    if rank == 0:\n",
    "        print (\"I'm rank zero and I'm sending a datum.\")\n",
    "        comm.Send(a[0], dest = 1) # Default destination is 0!\n",
    "    elif rank == 1:\n",
    "        print (\"I'm rank one and I'm receiving a datum.\")\n",
    "        comm.Recv(b, source = 0) # Default source is 0!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Parallel reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the previous examples, we used send and receive. Now we are going to look at some collective communication. These commands involve all the ranks that belong to a communicator. Let's start by creating an array of random numbers and scattering it to all the ranks, i.e., each rank gets some part of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Variables that are defined on the engines can be retrieved through the `view`. To get the variable rank, e.g., you can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ranks = view['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Until recently, the ranks were not in any particular order, which can be annoying, but we can get the right order by casting ranks to an ndarray and calling argsort. The result gives us the engines in MPI rank order. Give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ranksort = np.array(ranks).argsort()\n",
    "# rank0 = int(ranksort[0])\n",
    "rank0=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is executed within the notebook. The engines don't know anything about it\n",
    "import numpy as np\n",
    "a = np.random.random(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# First push the array to the engine with MPI rank 0\n",
    "%px a = None\n",
    "view.push({'a':a}, targets=rank0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Next, we scatter the data from rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "view.scatter('a_partial', a)\n",
    "view[\"a_partial\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "%%px\n",
    "a_partial = np.zeros(100000)\n",
    "comm.Scatter(a, a_partial, root = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can now calculate the partial sum on each rank and then sum up the partial results using `Reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "sum_partial = np.sum(a_partial)\n",
    "total = comm.reduce(sum_partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use the view to get the result back into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = view.pull('total', targets=rank0)\n",
    "sum_partial = view['sum_partial']\n",
    "print(\"The sum of the random numbers is %f. The average is %f.\" % (total, total / len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sum(sum_partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Upper vs. lowercase in mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`mpi4py` offers two version of many calls. The first one is written in uppercase. It uses memory buffers, e.g., `np.array`, and maps the call directly to the appropriate C call. The second version is written in lower case and takes arbitrary Python object. The result is given as the return value. Note, that for the uppercase versions all a_partial must have the same size! Unfortunately, the following code causes problems when run from the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "``` python\n",
    "a_all = None\n",
    "if rank == 0:\n",
    "    a_all = np.empty([size, len(a_partial)], dtype='f8')\n",
    "comm.Gather(a_partial, a_all, root=0)\n",
    "if rank == 0:\n",
    "    print(a_all)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "a_all = comm.gather(a_partial)\n",
    "if rank == 0:\n",
    "    print(a_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, `a_all` contains a `list` of `np.array`s.\n",
    "\n",
    "This second version is convenient, but it is **much** slower:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "``` python\n",
    "%%px\n",
    "%timeit a_all = comm.gather(a_partial)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It gets worse as arrays get bigger.\n",
    "\n",
    "To retrieve the result, we use the `view` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Remarks\n",
    "While Jupyter notebooks are a nice way to teach MPI and well tested MPI routines can be quite useful within a notebook, developing MPI code within notebooks can quickly become awkward because mistakes lead to blocking engines. I find it easier to write and test my MPI routines outside Jupyter notebooks and start the program with `mpiexec`/`srun` from the command line.\n",
    "\n",
    "For example, you could write the following program and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile helloparallelworld.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "numberOfRanks = MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "a = np.zeros(1)\n",
    "if rank == 0:\n",
    "    print(\"There are %d MPI ranks.\" % numberOfRanks)\n",
    "    a = np.random.random(1)\n",
    "\n",
    "print(\"I'm rank %d.\" % rank)\n",
    "if numberOfRanks < 2:\n",
    "    print(\"Not enough MPI tasks!\")\n",
    "    exit(1)\n",
    "if rank == 0:\n",
    "    MPI.COMM_WORLD.Send(a, dest=1)\n",
    "elif rank == 1:\n",
    "    MPI.COMM_WORLD.Recv(a, source=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can then switch to a terminal and execute it as `srun -n 2 python helloparallelworld.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Peak to Peak\n",
    "Write a program that generates random numbers from a Gaussian distribution and then finds the minimum and maximum number generated.\n",
    "\n",
    "a) Generate the random numbers on rank 0 and scatter them. Calculate the maximum and minimum for the partial data for    \n",
    "   each rank and send the results back to rank 0. Find the maximum and minimum on rank 0 and compare it with numpy's\n",
    "   `ptp` function.\n",
    "   \n",
    "b) Generate random numbers at each rank. Calculate the minimum and maximum and use `Reduce` to find the extrema and send \n",
    "   them to rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
